{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08efa8bc-e1a0-4727-9be3-f7c4bfd62d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.49.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (4.66.5)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision transformers tqdm sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e3a286-6da7-449a-88d5-74f155c781ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import transformers\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b7aec80-8277-4d81-a334-059a45184997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted dataset/Training/training_labels.csv to dataset/Training/training_labels.json\n",
      "Successfully converted dataset/Validation/validation_labels.csv to dataset/Validation/validation_labels.json\n",
      "Successfully converted dataset/Testing/testing_labels.csv to dataset/Testing/testing_labels.json\n"
     ]
    }
   ],
   "source": [
    "# Create json files to be read by model\n",
    "def make_json(csv_file_path, json_file_path):\n",
    "\n",
    "    data = {}\n",
    "    with open(csv_file_path, mode='r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            filename = row['IMAGE']\n",
    "            transcription = row['MEDICINE_NAME']\n",
    "            data[filename] = transcription\n",
    "    \n",
    "    with open(json_file_path, mode='w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"Successfully created {json_file_path}\")\n",
    "\n",
    "make_json('dataset/Training/training_labels.csv', 'dataset/Training/training_labels.json')\n",
    "make_json('dataset/Validation/validation_labels.csv', 'dataset/Validation/validation_labels.json')\n",
    "make_json('dataset/Testing/testing_labels.csv', 'dataset/Testing/testing_labels.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdbe0a71-cde6-4974-8876-3ff55a6e2178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_img_folder = \"dataset/Training/training_words\"\n",
    "train_annotations_file = \"dataset/Training/training_labels.json\"\n",
    "val_img_folder = \"dataset/Validation/validation_words\"\n",
    "val_annotations_file = \"dataset/Validation/validation_labels.json\"\n",
    "test_img_folder = \"dataset/Testing/testing_words\"\n",
    "test_annotations_file = \"dataset/Testing/testing_labels.json\"\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((296,296), \n",
    "    interpolation=T.InterpolationMode.BILINEAR),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "class DonutOCRDataset(Dataset):\n",
    "    def __init__(self, img_folder, annotations_file, transform):\n",
    "        self.img_folder = img_folder\n",
    "        self.transform = transform\n",
    "        with open(annotations_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.annotations = json.load(f)\n",
    "        self.image_files = list(self.annotations.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.image_files[idx]\n",
    "        img_path = os.path.join(self.img_folder, img_file)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error opening image {img_path}: {e}\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        target_text = self.annotations[img_file]\n",
    "        return image, target_text\n",
    "\n",
    "train_dataset = DonutOCRDataset(train_img_folder, train_annotations_file, transform)\n",
    "val_dataset = DonutOCRDataset(val_img_folder, val_annotations_file, transform)\n",
    "test_dataset = DonutOCRDataset(test_img_folder, test_annotations_file, transform)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, texts = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    tokenized = processor(\n",
    "        text=list(texts),\n",
    "        padding='max_length',  \n",
    "        truncation=True,       \n",
    "        max_length=512,     \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return images, tokenized.input_ids, tokenized.attention_mask\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72e66782-71a7-437e-9019-97b0a50dacb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, device, processor, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for images, labels, attention_mask in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values=images, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    loss = total_loss / len(train_loader)\n",
    "    val_accuracy = evaluate_accuracy(model, val_loader, device, processor)\n",
    "    print(f\"Epoch {epoch+1}: Loss: {loss}, Val Accuracy: {val_accuracy}\")\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, loader, device, processor):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets, _ in loader:  # Assuming collate_fn returns (images, input_ids, attention_mask)\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            generated_ids = model.generate(pixel_values=images)\n",
    "            # Decode predictions (list of strings)\n",
    "            preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            # Decode targets (convert tensor to list of strings)\n",
    "            target_strs = processor.batch_decode(targets, skip_special_tokens=True)\n",
    "            for pred, targ in zip(preds, target_strs):\n",
    "                if pred.strip().lower() == targ.strip().lower():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    model.train()\n",
    "    return correct / total if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7dff06ad-f5ff-4851-b651-12c2db25068e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 0.19403019092666607, Val Accuracy: 0.002564102564102564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss: 0.009468480019877927, Val Accuracy: 0.008974358974358974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss: 0.008908019508593358, Val Accuracy: 0.01282051282051282\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     18\u001b[0m     train(model, train_loader, val_loader, optimizer, device, processor, epoch)\n\u001b[0;32m---> 20\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m evaluate_accuracy(model, test_loader, device, processor)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnocom_donut_finetuned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"naver-clova-ix/donut-base\"\n",
    "processor = DonutProcessor.from_pretrained(model_name, use_fast=False)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
    "\n",
    "model.config.decoder_start_token_id = processor.tokenizer.eos_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    train(model, train_loader, val_loader, optimizer, device, processor, epoch)\n",
    "    \n",
    "test_accuracy = evaluate_accuracy(model, test_loader, device, processor)\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "\n",
    "model.save_pretrained(\"nocom_donut_finetuned\")\n",
    "processor.save_pretrained(\"nocom_donut_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543fa981-a4a3-442a-a265-587d1674f637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
